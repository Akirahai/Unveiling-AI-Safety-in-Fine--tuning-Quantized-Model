Evaluation Results for model: Llama-2-7B-Chat-GPTQ_samsumBad-7b-gptq-chat_final on samsum test dataset
Base model: TheBloke/Llama-2-7B-Chat-GPTQ
LoRA adapter: None
LoRA adapter name: samsum_adapter
Average Rouge F1 Score: 0.47632878008990365
Total samples evaluated: 200
Batch size used: 64
