{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8415c4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long_2/miniconda3/envs/hai_cot/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "import warnings\n",
    "from typing import List\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import LlamaConfig, LlamaTokenizer, LlamaForCausalLM,AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from safety_evaluation.eval_utils.prompt_utils import apply_prompt_template\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import GenerationConfig\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc24850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8ef14f960d454fbdbc89ac6c04cae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long_2/miniconda3/envs/hai_cot/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/long_2/miniconda3/envs/hai_cot/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/long_2/miniconda3/envs/hai_cot/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/long_2/miniconda3/envs/hai_cot/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = 'TheBloke/Llama-2-7B-Chat-fp16'\n",
    "# saved_peft_model_path = \"samsumBad-7b-gptq-chat_final\"\n",
    "# saved_peft_model_path = \"safeLora-samsumBad-7b-gptq-chat_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(path,\n",
    "                                            device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n",
    "                                             trust_remote_code=False, # prevents running custom model files on your machine\n",
    "                                             revision=\"main\") # which version of model to use in repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa618f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efef6d",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec218b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini =  \"You are a helpful, respectful and honest assistant. Your task is to summarize the following dialogue. Your answer should be based on the provided dialogue only.\"\n",
    "\n",
    "template = {\n",
    "    \"description\": \"Template used for PIQA dataset\",\n",
    "    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "    \"prompt_no_input\": \"You will be presented with the dialogue. Your goal is to summarize the following dialogue.\\n\\n### Instruction:{instruction}\\n\\n### Dialogue:{dialogue}\\n\\n ### Summary:\\n\",\n",
    "    \"response_split\": \"### Summary:\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a2e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini =  \"You are a helpful, respectful and honest assistant. Your task is to summarize the following dialogue. Your answer should be based on the provided dialogue only.\"\n",
    "\n",
    "template = {\n",
    "    \"description\": \"Template used for PIQA dataset\",\n",
    "    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "    \"prompt_no_input\": \"You are a helpful assistant for dialog summarization.\\n\\n### Dialogue:{dialogue}\\n\\n ### Summary:\\n\",\n",
    "    \"response_split\": \"### Summary:\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8ff4b",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf8f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test_path = \"datasets/samsum_test.jsonl\"\n",
    "\n",
    "def load_samsum_bad_data(file_path):\n",
    "    \"\"\"Load the samsum_1000_bad.jsonl data\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "test_data = load_samsum_bad_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90b0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = test_data[0]['messages'][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a8c00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be presented with the dialogue. Your goal is to summarize the following dialogue.\n",
      "\n",
      "### Instruction:You are a helpful, respectful and honest assistant. Your task is to summarize the following dialogue. Your answer should be based on the provided dialogue only.\n",
      "\n",
      "### Dialogue:Summary this dialog:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      " ### Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = template[\"prompt_no_input\"].format(\n",
    "                            instruction=ini, dialogue = question)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6cf3cd",
   "metadata": {},
   "source": [
    "## Evaluation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14007868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "def evaluate(res, ans):\n",
    "    if 'llama-3' in path or 'gemma' in path:\n",
    "        input_ids = tokenizer.apply_chat_template(res, add_generation_prompt=True)\n",
    "        input_ids= torch.tensor(input_ids).long()\n",
    "        input_ids= input_ids.unsqueeze(0)\n",
    "        input_ids= input_ids.to(\"cuda:0\")\n",
    "    else:\n",
    "\n",
    "        inputs = tokenizer(res, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )    \n",
    "            \n",
    "    s = generation_output.sequences[0]\n",
    "    if \"llama-3\" in path or \"gemma\" in path:\n",
    "        prediction = tokenizer.decode(s[input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    else:\n",
    "        output = tokenizer.decode(s)\n",
    "        # print(output)\n",
    "        prediction = output.split(template[\"response_split\"])[1].strip()\n",
    "        # prediction = prediction.split('</s>')[0].strip()\n",
    "        prediction = prediction.split('<|im_end|>')[0].strip()\n",
    "    print(\"Prediction:\", prediction)\n",
    "    print(\"Answer:\", ans)\n",
    "    results = rouge.compute(predictions=[prediction], references=[ans])\n",
    "    return results['rouge1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa0f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Betty's phone number is not available, so Amanda suggests asking Larry, who is a mutual friend. Amanda describes Larry as very nice, but Hannah is hesitant to reach out to him. Amanda encourages Hannah to text Larry, but Hannah is not interested. The conversation ends with a friendly goodbye.</s>\n",
      "Answer: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23880597014925373"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(prompt, test_data[0]['messages'][1]['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hai_cot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
